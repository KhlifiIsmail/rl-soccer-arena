# Training Configuration - SELF-PLAY MODE

# Environment Configuration
env:
  max_episode_steps: 2000
  time_step: 0.01
  reward_goal_scored: 100.0
  reward_goal_conceded: -50.0
  reward_own_goal: -100.0
  reward_ball_touch: 2.0
  reward_ball_to_goal: 5.0
  reward_no_action: 0.0

# Model Configuration (PPO)
model:
  learning_rate: 0.0003
  n_steps: 512
  batch_size: 128
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.1
  vf_coef: 0.5
  max_grad_norm: 0.5
  device: "cuda"

# Training Configuration
training:
  total_timesteps: 3000000
  n_envs: 8
  use_subprocess: false
  log_freq: 2048
  save_freq: 100000
  use_early_stopping: false

# Self-Play Configuration
self_play:
  enabled: true
  pool_size: 10
  save_freq: 100000
  update_freq: 10000
  selection_strategy: "uniform"

# General Settings
seed: 42
output_dir: "outputs"
